{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_multiple_whitespaces, stem_text\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download FastText pretrained vectors for English: \n",
    "[cc.en300.vec.gz](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
    "\n",
    "And download Yelp! dataset composed of reviews: \n",
    "[Yelp.train.text](https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded pretrained FastText vectors by gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = [\n",
    "    lambda word: word.lower(),   # Lowercase the word.\n",
    "    strip_multiple_whitespaces,  # Remove repeating whitespaces.\n",
    "    remove_stopwords             # Remove stopwords.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity of two words using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_vectors = KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization. Split Yelp! texts into separate tokens (words and punctuation marks) by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try part of speech tagging using [NLTK POS-tagger](https://www.nltk.org/book/ch05.html).\n",
    "The function returns list of tuples (word, POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the most similar word to the given? Can you write a method that returns a list of tuples (word, similarity) in order of decreasing similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the simplest reformulation task. We just want to reformulate some sentences replacing an ajective with a similar one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_sentence(sentence):\n",
    "    # Sentence tokenization\n",
    "    tokenized_sentence = tokenize_sentence(sentence)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    POS_tagged_words = POS_tagging(tokenized_sentence)\n",
    "\n",
    "    reformulated_sentence_words = []\n",
    "    for word, pos_tag in POS_tagged_words:\n",
    "        # If the word is adjective...\n",
    "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
    "            try:\n",
    "                # ...look for the word most similar to the given and replace it\n",
    "                # your code here\n",
    "            except:\n",
    "                print('There is no {} word in FastText dictionary! ...'.format(word))\n",
    "        else:\n",
    "            reformulated_sentence_words.append(word)\n",
    "    # Join words list in a sentence\n",
    "    return ' '.join(reformulated_sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER sentiment classifier from NLTK library. The range of sentiment is from -1 to 1 where -1 is negative, 0 is neutral and 1 is positive\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset text file line by line and put lines into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_list = sentiment_analyzer.lexicon_file.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Yelp dataset from text file and get 1000 random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Yelp.train.text', 'r') as f:\n",
    "    yelp_set = np.array(f.readlines())\n",
    "\n",
    "sentences = yelp_set[np.random.randint(0, 1000, (1000, 1))]\n",
    "\n",
    "processed_sentences = np.array([' '.join(preprocess_string(sentence[0], preprocessors)) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average sentiment of 1000 sentences sentences set by VADER sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0720283"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAverage(sentences):\n",
    "    scores = np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in sentences])\n",
    "    return scores.mean()\n",
    "\n",
    "getAverage(processed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformulate sentences and compute average sentiment again. Try to come up with ways to make senteces more positive on average. What about more negative? Can you come up with some interesting experiment on this data with POS-tagged reformulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service ok slow .' \"n't matter time .\" 'rude staff doesnt know .'\n",
      " 'came away experience leave .' 'service professional !'\n",
      " 'inside maybe worse , trash dirty .'\n",
      " 'better italian restaurants pittsburgh .' 'service awful .'\n",
      " \"'d comedians _num_ stars , food zero stars .\" 'eat , maybe try .'\n",
      " \"husband 's salad , italian chopped , better .\"\n",
      " 'disappointing easter dinner .' ', year crowd smaller smaller .'\n",
      " 'quickly ate left movie .' 'absolutely problems .'\n",
      " 'like white cheddar mash potatoes .' \"n't hungry actually tastebuds .\"\n",
      " 'staff rude .']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at words taht appear in positive sentences.\n",
    "positive_sentences = processed_sentences[scores > 0.5]\n",
    "print(positive_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['good', 'party'], dtype='<U9')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = np.array([preprocess_string(s, preprocessors)[0] for s in positive_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in positive_words])\n",
    "max_word = positive_words[all_scores.argmax()]\n",
    "positive_words = positive_words[all_scores > 0]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat .' 'important ... food terrible .' \", maybe n't care .\"\n",
      " 'time _num_ hours .' 'want chinese lee , china palace .'\n",
      " \"n't cash inside menu .\" \"n't try stay experience bad .\"\n",
      " \"n't know corporate keeps place open .\"\n",
      " 'sat bar female bartender miserable .'\n",
      " \"place worst dental offices 've walked .\" 'zero flavor dry hell .'\n",
      " 'took _num_ minutes mustard mayo .' ', suggest staying far , far away !'\n",
      " \"service n't better .\" 'lettuce drowning vinegar .'\n",
      " 'wait _num_ days seen .' 'spend money !'\n",
      " 'salad arrived , look like picture .' 'ordered past disappointed .'\n",
      " '_num_ got bar louie 8:30 .' 'place near waitresses .' 'confused ?'\n",
      " 'believe unprofessional store !' '_num_ minutes brought martini .'\n",
      " 'food cold bland , kids food cold .' ', certainly eat .' 'pinch .'\n",
      " 'went barnes noble waterfront _num_ books list .'\n",
      " '_num_ minutes brought martini .' '_num_ staff members yelled .'\n",
      " '_num_ .' \"food , , n't good .\" 'eat .'\n",
      " 'offered $ _num_ shirt shirts lost .' 'terrible .' 'ordinary .'\n",
      " 'needless , time going .' 'bad food , slow service rude managers .'\n",
      " \"walmart n't !\" 'lettuce drowning vinegar .'\n",
      " \", 're looking specific , good luck .\"\n",
      " 'highly disappointed service , ordered .' \"come -- `` asian bistro . ''\"\n",
      " 'thanks giving reason come .'\n",
      " 'trying healthy , ordered apple pecan salad .' 'horse play .'\n",
      " 'plus , _num_ new tires went flat _num_ weeks .'\n",
      " 'opinion , sing sing worth .' 'instead , total $ _num_ .'\n",
      " ', suggest staying far , far away !' 'skip .'\n",
      " 'regret leave 1/2 star rating .' 'fish old cold fries soft .'\n",
      " 'went tell wanted burger .' 'offer water , drink .' 'def worth wait .'\n",
      " 'service faster better .' 'rude paid ethics .'\n",
      " \"needless , n't stay eat went .\" 'service fast food terrible service .'\n",
      " '_num_ mins later refills .' 'wind washing hands getting salad stuff .'\n",
      " \"time seated dining area 's bad experience .\"\n",
      " 'time went chicken nuggets cooked .' ', ridiculous .' '.'\n",
      " 'food absolutely gross !' \"dirty paper towels women 's bathroom .\"\n",
      " 'outside breading hard rock taste .' 'ridiculous .' 'store bad service .'\n",
      " 'service pretty bad .' 'let tell bad experience .' 'pushed took chairs .'\n",
      " \"gut tells trust e 'n ps .\" 'location terrible .' 'people rude .'\n",
      " 'party _num_ adults _num_ kid .' \"'s ran problems .\"\n",
      " 'absolutely worst care experience vets !'\n",
      " \"'d comedians _num_ stars , food zero stars .\"\n",
      " 'visit took _num_ minutes waitresses attention .' \"'s ran problems .\"\n",
      " 'fish bland .' \"n't use services paid .\"\n",
      " 'outside breading hard rock taste .' 'corn lacked butter .'\n",
      " 'leave sore throat yelling heard .' \"n't word .\"\n",
      " \"'ll save $ $ , time , frustration !\" 'neutral' 'recommend sit eat .'\n",
      " 'going !' \"'s soooooo bland !\" 'eaten problems .'\n",
      " 'panera bread inconsistent restaurant .' 'lazy river better .' '_num_ .'\n",
      " 'sodas .']\n"
     ]
    }
   ],
   "source": [
    "# Words that appear in negative sentences.\n",
    "negative_sentences = processed_sentences[scores < -0.5]\n",
    "print(negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['confused', 'terrible', 'bad', 'regret', 'rude', 'dirty',\n",
       "       'ridiculous', 'leave', 'lazy'], dtype='<U10')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = np.array([preprocess_string(s, preprocessors)[0] for s in negative_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in negative_words])\n",
    "min_word = negative_words[all_scores.argmin()]\n",
    "negative_words = negative_words[all_scores < 0]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0925618\n"
     ]
    }
   ],
   "source": [
    "def replace(sentence, old_words, new_word):\n",
    "    words = preprocess_string(sentence, preprocessors)\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(new_word if w in old_words else w)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "    \n",
    "# If we replace positive words by negatives we make sentences more negative.\n",
    "new_processed_sentences = np.array([replace(sentence, positive_words, min_word) for sentence in processed_sentences])\n",
    "print(getAverage(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018853000000000008\n"
     ]
    }
   ],
   "source": [
    "# If we replace negative words by positive we make sentences more positive.\n",
    "new_processed_sentences = np.array([replace(sentence, negative_words, max_word) for sentence in processed_sentences])\n",
    "print(getAverage(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
