{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_multiple_whitespaces, \\\n",
    "                                         strip_punctuation, stem_text\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download FastText pretrained vectors for English: \n",
    "[cc.en300.vec.gz](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
    "\n",
    "And download Yelp! dataset composed of reviews: \n",
    "[Yelp.train.text](https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded pretrained FastText vectors by gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "! gunzip -k cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity of two words using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10155682\n"
     ]
    }
   ],
   "source": [
    "fname = 'cc.en.300.vec'\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(fname)\n",
    "print(word_vectors.similarity('king', 'egg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization. Split Yelp! texts into separate tokens (words and punctuation marks) by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Yelp.train.text', 'r') as f:\n",
    "    yelp_set = np.array(f.readlines())\n",
    "\n",
    "preprocessors = [\n",
    "    lambda word: word.lower(),   # Lowercase the word.\n",
    "    strip_multiple_whitespaces,  # Remove repeating whitespaces.\n",
    "]\n",
    "\n",
    "tokenize_sentence = lambda x: preprocess_string(x, preprocessors)\n",
    "\n",
    "tokens = np.array([tokenize_sentence(sentence) for sentence in yelp_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wa', 'sadli', 'mistaken']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try part of speech tagging using [NLTK POS-tagger](https://www.nltk.org/book/ch05.html).\n",
    "The function returns list of tuples (word, POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'sadly', 'mistaken', '.']\n",
      "[('i', 'NN'), ('was', 'VBD'), ('sadly', 'RB'), ('mistaken', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "\n",
    "def POS_tagging(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "print(POS_tagging(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the most similar word to the given? Can you write a method that returns a list of tuples (word, similarity) in order of decreasing similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bowsers', 0.7382202744483948),\n",
       " ('koopa', 0.47836795449256897),\n",
       " ('Fawful', 0.44402459263801575),\n",
       " ('koopas', 0.43662241101264954),\n",
       " ('Bowser', 0.43195170164108276),\n",
       " ('FLUDD', 0.4224753677845001),\n",
       " ('mario', 0.4222983717918396),\n",
       " ('waluigi', 0.4180498719215393),\n",
       " ('Koopa', 0.4134894013404846),\n",
       " ('WHOAAAAAA', 0.41194313764572144)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_n = lambda word, topn: word_vectors.most_similar(word, topn=topn)\n",
    "\n",
    "most_similar_n('bowser', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the simplest reformulation task. We just want to reformulate some sentences replacing an ajective with a similar one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal meat and a ton of shredded lettuce .\n",
      "\n",
      "minim meat and a ton of shread lettuc\n"
     ]
    }
   ],
   "source": [
    "def reformulate_sentence(sentence):\n",
    "    # Sentence tokenization\n",
    "    tokenized_sentence = tokenize_sentence(sentence)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    POS_tagged_words = POS_tagging(tokenized_sentence)\n",
    "\n",
    "    reformulated_sentence_words = []\n",
    "    for word, pos_tag in POS_tagged_words:\n",
    "        # If the word is adjective...\n",
    "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
    "            try:\n",
    "                new_word = most_similar_n(word, 1)[0][0]\n",
    "                reformulated_sentence_words.append(new_word)\n",
    "            except:\n",
    "                print('There is no {} word in FastText dictionary! ...'.format(word))\n",
    "        else:\n",
    "            reformulated_sentence_words.append(word)\n",
    "    # Join words list in a sentence\n",
    "    return ' '.join(reformulated_sentence_words)\n",
    "\n",
    "idx = 2\n",
    "print(yelp_set[idx])\n",
    "print(reformulate_sentence(yelp_set[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER sentiment classifier from NLTK library. The range of sentiment is from -1 to 1 where -1 is negative, 0 is neutral and 1 is positive\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset text file line by line and put lines into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Not clear why should we read it.\n",
    "lexicon_list = sentiment_analyzer.lexicon_file.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Yelp dataset from text file and get 1000 random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = yelp_set[np.random.randint(0, 1000, (1000, 1))]\n",
    "\n",
    "processed_sentences = np.array([' '.join(preprocess_string(sentence[0], preprocessors)) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average sentiment of 1000 sentences sentences set by VADER sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1151995"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average = lambda sentences: np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in sentences]).mean()\n",
    "\n",
    "get_average(processed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformulate sentences and compute average sentiment again. Try to come up with ways to make senteces more positive on average. What about more negative? Can you come up with some interesting experiment on this data with POS-tagged reformulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this beer is like champaign and it makes my whole stomach nice and cold .'\n",
      " \"that 's where the praise ends .\"\n",
      " 'ambiance , like i said , left much to be desired .'\n",
      " 'i really really want this place to do better .'\n",
      " 'while sitting there we noticed _num_ other parties walked out as well .'\n",
      " \"but good lord , they 're just so loud !\"\n",
      " 'thanks for giving me one more reason never to come back .'\n",
      " 'too many other good choices nearby to give them a second chance - sorry .'\n",
      " \"but that 's not the worst .\"\n",
      " \"but yes , you guessed it , that 's overpriced too .\"]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at words that appear in positive sentences.\n",
    "scores = np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in processed_sentences])\n",
    "positive_sentences = processed_sentences[scores > 0.5]\n",
    "print(positive_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thanks'], dtype='<U8')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = np.array([preprocess_string(s, preprocessors)[0] for s in positive_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in positive_words])\n",
    "max_word = positive_words[all_scores.argmax()]\n",
    "positive_words = positive_words[all_scores > 0]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they completely ignored us so we got up and left .'\n",
      " 'the service has also suffered tremendously .'\n",
      " 'poor service from the door .' 'this time , it was ridiculous .'\n",
      " 'the service was terrible .'\n",
      " 'the second was passable , but nothing special .'\n",
      " \"the food was n't very good at all .\" 'the food was lousy .'\n",
      " 'but hour plus for a chicken sandwich is unacceptable .' 'no apology .']\n"
     ]
    }
   ],
   "source": [
    "# Words that appear in negative sentences.\n",
    "negative_sentences = processed_sentences[scores < -0.2]\n",
    "print(negative_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['poor', 'no', 'disappointing', 'poor', 'apathetic', 'bad', 'bad',\n",
       "       'appalling', 'no', 'disgusting', 'terrible', 'worst', 'horrible',\n",
       "       'unfortunately', 'terrible', 'horrible', 'worst', 'unfortunately',\n",
       "       'terrible', 'appalling', 'no', 'unfortunately', 'horrible',\n",
       "       'terrible', 'no', 'worst', 'awful', 'no', 'no', 'avoid', 'stopped',\n",
       "       'no', 'worst', 'worst', 'terrible', 'lame', 'no', 'terrible',\n",
       "       'worst', 'horrible', 'sadly', 'wrong', 'no', 'nah', 'bad',\n",
       "       'terrible', 'unfortunately', 'terrible', 'no', 'worst', 'no', 'no',\n",
       "       'terrible', 'stopped', 'disgusted', 'unfortunately',\n",
       "       'unfortunately', 'avoid', 'no', 'disappointing', 'worst', 'no',\n",
       "       'horrible', 'bad', 'rob', 'unfortunately', 'no', 'no', 'worst',\n",
       "       'worst', 'no', 'no', 'unfortunately'], dtype='<U13')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = np.array([preprocess_string(s, preprocessors)[0] for s in negative_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in negative_words])\n",
    "min_word = negative_words[all_scores.argmin()]\n",
    "negative_words = negative_words[all_scores < 0]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11454839999999998\n"
     ]
    }
   ],
   "source": [
    "def replace(sentence, old_words, new_word):\n",
    "    words = preprocess_string(sentence, preprocessors)\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(new_word if w in old_words else w)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "# If we replace positive words by negatives we make sentences more negative.\n",
    "new_processed_sentences = np.array([replace(sentence, positive_words, min_word) for sentence in processed_sentences])\n",
    "print(get_average(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04678070000000001\n"
     ]
    }
   ],
   "source": [
    "# If we replace negative words by positive we make sentences more positive.\n",
    "new_processed_sentences = np.array([replace(sentence, negative_words, max_word) for sentence in processed_sentences])\n",
    "print(get_average(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
