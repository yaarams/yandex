{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mariao/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import warnings\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_multiple_whitespaces, \\\n",
    "                                         strip_punctuation, stem_text\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download FastText pretrained vectors for English: \n",
    "[cc.en300.vec.gz](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
    "\n",
    "And download Yelp! dataset composed of reviews: \n",
    "[Yelp.train.text](https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded pretrained FastText vectors by gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "! gunzip -k cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity of two words using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10155682\n"
     ]
    }
   ],
   "source": [
    "fname = 'cc.en.300.vec'\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(fname)\n",
    "print(word_vectors.similarity('king', 'egg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization. Split Yelp! texts into separate tokens (words and punctuation marks) by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_file = 'Yelp.train.text'\n",
    "\n",
    "with open(yelp_file, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "tokens = list(map(word_tokenize, lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'ordered',\n",
       "  'it',\n",
       "  'without',\n",
       "  'lettuce',\n",
       "  ',',\n",
       "  'tomato',\n",
       "  ',',\n",
       "  'onions',\n",
       "  ',',\n",
       "  'or',\n",
       "  'dressing',\n",
       "  '.'],\n",
       " ['are', 'you', 'kidding', 'me', '?']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[9:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try part of speech tagging using [NLTK POS-tagger](https://www.nltk.org/book/ch05.html).\n",
    "The function returns list of tuples (word, POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'sadly', 'mistaken', '.']\n",
      "[('i', 'NN'), ('was', 'VBD'), ('sadly', 'RB'), ('mistaken', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "\n",
    "def POS_tagging(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "print(POS_tagging(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the most similar word to the given? Can you write a method that returns a list of tuples (word, similarity) in order of decreasing similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bowsers', 0.7382202744483948),\n",
       " ('koopa', 0.47836795449256897),\n",
       " ('Fawful', 0.44402459263801575),\n",
       " ('koopas', 0.43662241101264954),\n",
       " ('Bowser', 0.43195170164108276),\n",
       " ('FLUDD', 0.4224753677845001),\n",
       " ('mario', 0.4222983717918396),\n",
       " ('waluigi', 0.4180498719215393),\n",
       " ('Koopa', 0.4134894013404846),\n",
       " ('WHOAAAAAA', 0.41194313764572144)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_n = lambda word, topn: word_vectors.most_similar(word, topn=topn)\n",
    "\n",
    "most_similar_n('bowser', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the simplest reformulation task. We just want to reformulate some sentences replacing an ajective with a similar one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal meat and a ton of shredded lettuce .\n",
      "minmal meat and a ton of shreaded lettuce .\n"
     ]
    }
   ],
   "source": [
    "def reformulate_sentence(sentence):\n",
    "    # Sentence tokenization\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    POS_tagged_words = POS_tagging(tokenized_sentence)\n",
    "\n",
    "    reformulated_sentence_words = []\n",
    "    for word, pos_tag in POS_tagged_words:\n",
    "        # If the word is adjective...\n",
    "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
    "            try:\n",
    "                new_word = most_similar_n(word, 1)[0][0]\n",
    "                reformulated_sentence_words.append(new_word)\n",
    "            except:\n",
    "                print('There is no {} word in FastText dictionary! ...'.format(word))\n",
    "        else:\n",
    "            reformulated_sentence_words.append(word)\n",
    "    # Join words list in a sentence\n",
    "    return ' '.join(reformulated_sentence_words)\n",
    "\n",
    "idx = 2\n",
    "print(lines[idx])\n",
    "print(reformulate_sentence(lines[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER sentiment classifier from NLTK library. The range of sentiment is from -1 to 1 where -1 is negative, 0 is neutral and 1 is positive\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset text file line by line and put lines into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(yelp_file, 'r') as f:\n",
    "    lines = np.array([line for line in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Yelp dataset from text file and get 1000 random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = [\n",
    "    lambda word: word.lower(),   # Lowercase the word.\n",
    "    strip_multiple_whitespaces,  # Remove repeating whitespaces.\n",
    "]\n",
    "\n",
    "sentences = lines[np.random.randint(0, 1000, (1000, 1))]\n",
    "\n",
    "processed_sentences = np.array([' '.join(preprocess_string(sentence[0], preprocessors)) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average sentiment of 1000 sentences sentences set by VADER sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1232498"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average = lambda sentences: np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in sentences]).mean()\n",
    "\n",
    "get_average(processed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformulate sentences and compute average sentiment again. Try to come up with ways to make senteces more positive on average. What about more negative? Can you come up with some interesting experiment on this data with POS-tagged reformulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the coffee at this place was brewed to perfection which a appreciate .'\n",
      " 'this place is huge and was moderately clean except the bathrooms .'\n",
      " 'the music the guys played was all stuff from like _num_ .'\n",
      " 'i really really want this place to do better .'\n",
      " \"but that 's not the worst .\"\n",
      " 'while sitting there we noticed _num_ other parties walked out as well .'\n",
      " '- none of the food was super awesome .'\n",
      " \"that 's where the praise ends .\"\n",
      " '- none of the food was super awesome .'\n",
      " \"however , if you 're looking for something specific , good luck .\"]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at words that appear in positive sentences.\n",
    "scores = np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in processed_sentences])\n",
    "positive_sentences = processed_sentences[scores > 0.5]\n",
    "print(positive_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U8')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = np.array([preprocess_string(s, preprocessors)[0] for s in positive_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in positive_words])\n",
    "max_word = positive_words[all_scores.argmax()]\n",
    "positive_words = positive_words[all_scores > 0]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely no problems .' 'terrible service .'\n",
      " 'this store has such bad service .'\n",
      " 'very disappointing considering their pricing structure .'\n",
      " 'there is no excuse for the poor service and food .'\n",
      " \"it 's the customers fault your staff ca n't handle a sat night rush .\"\n",
      " 'i was extremely disappointed by the lack of presentation .'\n",
      " 'was so salty and just not good .'\n",
      " 'the waitress there are extremely rude .'\n",
      " 'also , i wrote to the restaurant and received no response .']\n"
     ]
    }
   ],
   "source": [
    "# Words that appear in negative sentences.\n",
    "negative_sentences = processed_sentences[scores < -0.2]\n",
    "print(negative_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['terrible', 'avoid', 'worst', 'no', 'disappointing', 'no', 'avoid',\n",
       "       'annoying', 'awful', 'avoid', 'no', 'no', 'worst', 'no',\n",
       "       'unfortunately', 'unfortunately', 'apathetic', 'poor', 'sadly',\n",
       "       'appalling', 'ugh', 'terrible', 'terrible', 'bad', 'no', 'no',\n",
       "       'unfortunately', 'disappointing', 'disgusting', 'poor', 'annoying',\n",
       "       'awful', 'no', 'rob', 'gross', 'disappointment', 'worst', 'awful',\n",
       "       'disgusting', 'ugh', 'no', 'noisy', 'noisy', 'worst', 'ugh',\n",
       "       'stopped', 'ugh', 'no', 'annoying', 'worst', 'worst', 'dirty',\n",
       "       'terrible', 'gross', 'avoid', 'ugh', 'ugh', 'noisy', 'no', 'no',\n",
       "       'terrible', 'no', 'appalling', 'avoid', 'ugh', 'no', 'bad',\n",
       "       'ridiculous', 'nah', 'disgusting', 'rob', 'unfortunately', 'ugh',\n",
       "       'no', 'worst', 'unfortunately', 'awful', 'rob', 'worst', 'ugh',\n",
       "       'poor'], dtype='<U14')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = np.array([preprocess_string(s, preprocessors)[0] for s in negative_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in negative_words])\n",
    "min_word = negative_words[all_scores.argmin()]\n",
    "negative_words = negative_words[all_scores < 0]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1232498\n"
     ]
    }
   ],
   "source": [
    "def replace(sentence, old_words, new_word):\n",
    "    words = preprocess_string(sentence, preprocessors)\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(new_word if w in old_words else w)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "# If we replace positive words by negatives we make sentences more negative.\n",
    "new_processed_sentences = np.array([replace(sentence, positive_words, min_word) for sentence in processed_sentences])\n",
    "print(get_average(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03595439999999999\n"
     ]
    }
   ],
   "source": [
    "# If we replace negative words by positive we make sentences more positive.\n",
    "new_processed_sentences = np.array([replace(sentence, negative_words, max_word) for sentence in processed_sentences])\n",
    "print(get_average(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
