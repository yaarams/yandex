{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_multiple_whitespaces, stem_text\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download FastText pretrained vectors for English: \n",
    "[cc.en300.vec.gz](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
    "\n",
    "And download Yelp! dataset composed of reviews: \n",
    "[Yelp.train.text](https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded pretrained FastText vectors by gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Yelp.train.text', 'r') as f:\n",
    "    yelp_set = np.array(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity of two words using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10155682\n"
     ]
    }
   ],
   "source": [
    "fname = 'cc.en.300.vec'\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(fname, limit=5000)\n",
    "print(word_vectors.similarity('king', 'egg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization. Split Yelp! texts into separate tokens (words and punctuation marks) by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = [\n",
    "    lambda word: word.lower(),   # Lowercase the word.\n",
    "    strip_multiple_whitespaces,  # Remove repeating whitespaces.\n",
    "]\n",
    "\n",
    "tokenize_sentence = lambda x: preprocess_string(x, preprocessors)\n",
    "\n",
    "tokens = np.array([tokenize_sentence(sentence) for sentence in yelp_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'was', 'sadly', 'mistaken', '.']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try part of speech tagging using [NLTK POS-tagger](https://www.nltk.org/book/ch05.html).\n",
    "The function returns list of tuples (word, POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'sadly', 'mistaken', '.']\n",
      "[('i', 'NN'), ('was', 'VBD'), ('sadly', 'RB'), ('mistaken', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "\n",
    "def POS_tagging(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "print(POS_tagging(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the most similar word to the given? Can you write a method that returns a list of tuples (word, similarity) in order of decreasing similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phrase', 0.7383495569229126),\n",
       " ('words', 0.7039700150489807),\n",
       " ('meaning', 0.6170327067375183),\n",
       " ('Word', 0.5634478330612183),\n",
       " ('term', 0.4986676871776581),\n",
       " ('sentence', 0.4960007667541504),\n",
       " ('name', 0.48936933279037476),\n",
       " ('definition', 0.4658313989639282),\n",
       " ('describe', 0.45316362380981445),\n",
       " ('letter', 0.44678452610969543)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def returnTopN(word, N):\n",
    "    return word_vectors.most_similar('word', topn=N)\n",
    "\n",
    "returnTopN('quit', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the simplest reformulation task. We just want to reformulate some sentences replacing an ajective with a similar one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ee'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformulate_sentence(sentence):\n",
    "    # Sentence tokenization\n",
    "    tokenized_sentence = tokenize_sentence(sentence)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    POS_tagged_words = POS_tagging(tokenized_sentence)\n",
    "\n",
    "    reformulated_sentence_words = []\n",
    "    for word, pos_tag in POS_tagged_words:\n",
    "        # If the word is adjective...\n",
    "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
    "            try:\n",
    "                reformulated_sentence_words.append(returnTopN(word, 1)[0])\n",
    "                # TODO: ...look for the word most similar to the given and replace it\n",
    "                print('')\n",
    "            except:\n",
    "                print('There is no {} word in FastText dictionary! ...'.format(word))\n",
    "        else:\n",
    "            reformulated_sentence_words.append(word)\n",
    "    # Join words list in a sentence\n",
    "    return ' '.join(reformulated_sentence_words)\n",
    "\n",
    "reformulate_sentence('ee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mariao/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER sentiment classifier from NLTK library. The range of sentiment is from -1 to 1 where -1 is negative, 0 is neutral and 1 is positive\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset text file line by line and put lines into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_list = sentiment_analyzer.lexicon_file.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Yelp dataset from text file and get 1000 random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = [\n",
    "    lambda word: word.lower(),   # Lowercase the word.\n",
    "    strip_multiple_whitespaces,  # Remove repeating whitespaces.\n",
    "    remove_stopwords             # Remove stopwords.\n",
    "]\n",
    "\n",
    "sentences = yelp_set[np.random.randint(0, 1000, (1000, 1))]\n",
    "\n",
    "processed_sentences = np.array([' '.join(preprocess_string(sentence[0], preprocessors)) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average sentiment of 1000 sentences sentences set by VADER sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0671777"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAverage(sentences):\n",
    "    scores = np.array([sentiment_analyzer.polarity_scores(s)['compound'] for s in sentences])\n",
    "    return scores.mean()\n",
    "\n",
    "getAverage(processed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformulate sentences and compute average sentiment again. Try to come up with ways to make senteces more positive on average. What about more negative? Can you come up with some interesting experiment on this data with POS-tagged reformulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['took _num_ minutes acknowledged .' \"n't manager 10:30 pm .\"\n",
      " 'brought pile beverage napkins .' 'server nice , issue .'\n",
      " 'completely ignored got left .' '_num_ .' '_num_ .'\n",
      " 'bit confusing layout , tastefully .' ', better skipping .' 'terrible .'\n",
      " 'service terrible .' 'door signage ( open hours ) .'\n",
      " 'wind washing hands getting salad stuff .' 'lobster bisque soup good .'\n",
      " 'wake going lose business .' \"'s tiny long bar like old irish pubs .\"\n",
      " 'pretty hard mess salad .'\n",
      " 'poor management skills rude behavior ruined holiday .']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at words taht appear in positive sentences.\n",
    "positive_sentences = processed_sentences[scores > 0.5]\n",
    "print(positive_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pretty'], dtype='<U10')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = np.array([preprocess_string(s, preprocessors)[0] for s in positive_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in positive_words])\n",
    "max_word = positive_words[all_scores.argmax()]\n",
    "positive_words = positive_words[all_scores > 0]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disappointed .' 'mushroom cheese omelette , cheese lacking .'\n",
      " 'sat _num_ minutes server came table .'\n",
      " 'minimal meat ton shredded lettuce .' \"`` nah '' _num_ .\" 'hell !'\n",
      " 'bf got lost pittsburgh _num_ min .' \"n't word .\"\n",
      " \"service n't bad , better .\" \", maybe n't care .\"]\n"
     ]
    }
   ],
   "source": [
    "# Words that appear in negative sentences.\n",
    "negative_sentences = processed_sentences[scores < -0.2]\n",
    "print(negative_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disappointed', 'hell', 'awful', 'lame', 'crap', 'awful',\n",
       "       'disappointed', 'disappointed', 'unfortunately', 'bad',\n",
       "       'disappointing', 'appalling', 'misses', 'worst', 'ridiculous',\n",
       "       'disgusting', 'lame', 'avoid', 'hell', 'noisy', 'low', 'wrong',\n",
       "       'apathetic', 'worst', 'uncomfortable', 'unprofessional', 'rude',\n",
       "       'poor', 'unprofessional', 'unfortunately', 'poor', 'terrible',\n",
       "       'awful', 'complaints', 'bad', 'complaints', 'worst',\n",
       "       'unfortunately', 'awful', 'disgusting', 'avoid', 'worst',\n",
       "       'smothered', 'frustrated', 'worst', 'hate', 'worst', 'stopped',\n",
       "       'meh', 'bad'], dtype='<U14')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = np.array([preprocess_string(s, preprocessors)[0] for s in negative_sentences]).flatten()\n",
    "all_scores = np.array([sentiment_analyzer.polarity_scores(w)['compound'] for w in negative_words])\n",
    "min_word = negative_words[all_scores.argmin()]\n",
    "negative_words = negative_words[all_scores < 0]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0812027\n"
     ]
    }
   ],
   "source": [
    "def replace(sentence, old_words, new_word):\n",
    "    words = preprocess_string(sentence, preprocessors)\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(new_word if w in old_words else w)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "    \n",
    "# If we replace positive words by negatives we make sentences more negative.\n",
    "new_processed_sentences = np.array([replace(sentence, positive_words, min_word) for sentence in processed_sentences])\n",
    "print(getAverage(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1258009\n"
     ]
    }
   ],
   "source": [
    "# If we replace negative words by positive we make sentences more positive.\n",
    "new_processed_sentences = np.array([replace(sentence, negative_words, max_word) for sentence in processed_sentences])\n",
    "print(getAverage(new_processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
